{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 21 Solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6220769",
   "metadata": {},
   "outputs": [],
   "source": [
    "The estimated depth of a Decision Tree trained on a one million instance training set can vary depending on the complexity of \n",
    "the data and the specific algorithm used. However, it is generally expected that the depth of the tree will be relatively \n",
    "deep, potentially reaching several levels, to capture the intricacies of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Gini impurity of a node is typically lower than or equal to that of its parent node. It is not always lower or always greater, \n",
    "as it depends on the specific splitting criterion and the distribution of classes within the node. However, in most cases, the Gini \n",
    "impurity tends to decrease as we move from the parent node to its child nodes, as the goal of splitting is to create subsets with \n",
    "more homogeneous class distributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reducing the maximum depth of a Decision Tree can be a good idea if the model is overfitting the training set. \n",
    "Overfitting occurs when the tree becomes too complex, capturing noise and outliers in the data instead of general patterns. \n",
    "By reducing the maximum depth, we limit the tree's ability to create complex and detailed splits, which helps prevent overfitting.\n",
    "\n",
    "A shallower tree with a reduced maximum depth tends to have higher bias and lower variance. It may sacrifice some of the model's\n",
    "ability to capture intricate patterns in the training data but can improve its ability to generalize to unseen data.\n",
    "It can result in a simpler and more interpretable model that avoids overfitting and performs better on test data or new observations.\n",
    "\n",
    "However, the optimal maximum depth depends on the specific dataset and problem. It is important to find the right balance between \n",
    "underfitting and overfitting by tuning the hyperparameters and evaluating the model's performance on a validation or test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaling the input features is generally not necessary or beneficial for a Decision Tree if it is underfitting the training set. \n",
    "Decision Trees are not sensitive to the scale of the input features because they make decisions based on feature thresholds rather \n",
    "than the actual feature values.\n",
    "\n",
    "Underfitting occurs when the Decision Tree is too simple and fails to capture the underlying patterns in the data. \n",
    "Scaling the input features does not address this issue because it does not affect the structure or complexity of the tree.\n",
    "\n",
    "Instead of scaling the features, other approaches should be considered to address underfitting in a Decision Tree. \n",
    "Some possible solutions include increasing the maximum depth of the tree, allowing more splits and complexity, adding \n",
    "more relevant features, or using ensemble techniques such as Random Forests to combine multiple trees.\n",
    "\n",
    "However, it's important to note that if there are other algorithms or models in the pipeline that rely on scaled features, \n",
    "then scaling may be necessary for consistency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d668628",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assuming that the time to train a Decision Tree is directly proportional to the size of the training set, \n",
    "we can estimate the time it will take to train a Decision Tree on a training set of 10 million instances.\n",
    "\n",
    "If it takes 1 hour to train a Decision Tree on a training set of 1 million instances, we can use the concept of proportionality\n",
    "to estimate the time for a 10 million-instance training set.\n",
    "\n",
    "The ratio between the training set sizes is 10 million / 1 million = 10. So, we can assume that training a Decision Tree on a 10 \n",
    "million-instance training set will take approximately 10 times longer than training on a 1 million-instance training set.\n",
    "\n",
    "Therefore, it can be estimated that training another Decision Tree on a training set of 10 million instances would take approximately \n",
    "10 hours. However, it's important to note that this is a rough estimate, and the actual training time can be influenced by various\n",
    "factors such as hardware capabilities, optimization techniques, and complexity of the data and model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Will setting presort=True speed up training if your training set has 100,000 instances ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e62081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Setting presort=True in a Decision Tree algorithm specifies that the training data should be presorted to improve the speed of training.\n",
    "However, whether it will actually speed up training depends on the specific characteristics of the dataset and the implementation of \n",
    "the algorithm.\n",
    "\n",
    "For smaller datasets, such as the one with 100,000 instances mentioned in the question, the overhead of presorting the data may outweigh \n",
    "the potential speed improvements. Presorting requires additional computational resources and time upfront to sort the data,\n",
    "which may not be efficient for smaller datasets.\n",
    "\n",
    "In general, the decision to use presort=True should be based on the size and complexity of the dataset, the available computational \n",
    "resources, and the specific implementation of the algorithm. It is recommended to try training with and without presorting and compare \n",
    "the training times to determine the effectiveness of using presort=True for a particular dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "1. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "2. Divide the dataset into a training and a test collection with train test split().\n",
    "3. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "4. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ebbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step a: Create the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "\n",
    "# Step b: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step c: Perform grid search to find the best hyperparameters\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Step d: Train the model with the best hyperparameters on the entire training set\n",
    "clf = DecisionTreeClassifier(**best_params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. Follow these steps to grow a forest:\n",
    "1. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "2. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision        Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "3. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "4. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Step a: Create subsets of the training set\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "rs = ShuffleSplit(n_splits=1000, train_size=100, random_state=42)\n",
    "subsets = []\n",
    "\n",
    "for train_index, _ in rs.split(X):\n",
    "    X_subset, y_subset = X[train_index], y[train_index]\n",
    "    subsets.append((X_subset, y_subset))\n",
    "\n",
    "# Step b: Train Decision Trees on each subset and evaluate on the test set\n",
    "dtrees = []\n",
    "accuracies = []\n",
    "\n",
    "for X_subset, y_subset in subsets:\n",
    "    dtree = DecisionTreeClassifier(**best_params)\n",
    "    dtree.fit(X_subset, y_subset)\n",
    "    dtrees.append(dtree)\n",
    "    accuracy = dtree.score(X_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Step c: Make predictions using the ensemble of Decision Trees\n",
    "predictions = np.array([dtree.predict(X_test) for dtree in dtrees])\n",
    "ensemble_predictions = mode(predictions, axis=0)[0]\n",
    "\n",
    "# Step d: Evaluate the ensemble predictions on the test set\n",
    "ensemble_accuracy = np.mean(ensemble_predictions == y_test)\n",
    "improvement = ensemble_accuracy - accuracy\n",
    "\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "print(\"Accuracy Improvement:\", improvement)\n",
    "\n",
    "\n",
    "\n",
    "Ensemble Accuracy: 0.871\n",
    "Accuracy Improvement: 0.039000000000000035\n",
    "/tmp/ipykernel_4781/381836745.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
    "  ensemble_predictions = mode(predictions, axis=0)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
