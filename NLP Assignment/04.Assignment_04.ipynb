{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 04 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tCan you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79783c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length.\n",
    "\n",
    "Applications are speech recognition, machine translation, image captioning and question answering.\n",
    "\n",
    "A variable-length context vector can be used instead of a ﬁxed-size vector. An Attention mechanism can be used to produces a sequence of vectors from the encoder RNN from each time step of the input sequence. The Decoder learns to pay selective attention to the vectors to produce the output at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhy do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d250f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "This two-step model, called an Encoder–Decoder, works much better than trying to translate on the fly with a single sequence-to-sequence RNN (like the one represented on the top left), since the last words of a sentence can affect the first words of the translation, so you need to wait until you have heard the whole sentence before translating it. Seq-2-seq RNNs translate one word at a time where as encoder-decoder RNNs read & translate a sentence at a time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tHow could you combine a convolutional neural network with an RNN to classify videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can combine CNN and RNN.\n",
    "\n",
    "Each video is converted into sequential images and passed onto the CNN to extract spatial features. The outputs are then passed into a recurrent sequence learning model (i.e. LSTM) to identify temporal features within the image sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tWhat are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "Below are the advantages of building an RNN using dynamic_rnn() rather than static_rnn():\n",
    "\n",
    "Avoids out-of-memory errors\n",
    "\n",
    "Directly takes single tensor as input and output (covering all time steps)\n",
    "\n",
    "No need to stack, unstack, or transpose\n",
    "\n",
    "Generates a smaller easier to visualize graph in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tHow can you deal with variable-length input sequences? What about variable-length output sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32071284",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can deal with varibale-length input sequences in the following ways:\n",
    "\n",
    "Set sequence_length parameter when calling static_rnn() or dynamic_rnn()\n",
    "\n",
    "Pad smaller input/output to make them same size as largest input/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is a common way to distribute training and execution of a deep RNN across multiple GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Common way to distribute training and execution of a deep RNN across multiple GPUs is to place each layer on a different GPU.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
